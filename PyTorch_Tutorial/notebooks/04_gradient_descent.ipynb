{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "### PART 1\n",
    "\n",
    "- Prediction MANUALLY \n",
    "- Gradients Computation MANUALLY\n",
    "- Loss Computation MANUALLY\n",
    "- Parameter Update MANUALLY\n",
    "\n",
    "### PART 2\n",
    "\n",
    "- Prediction MANUALLY \n",
    "- Gradients Computation **Autograd**\n",
    "- Loss Computation MANUALLY\n",
    "- Parameter Update MANUALLY\n",
    "\n",
    "### PART 3\n",
    "\n",
    "- Prediction MANUALLY \n",
    "- Gradients Computation **Autograd**\n",
    "- Loss Computation **PyTorch Loss**\n",
    "- Parameter Update **PyTorch Optimizer**\n",
    "\n",
    "### PART 4\n",
    "\n",
    "- Prediction **PyTorch Model** \n",
    "- Gradients Computation **Autograd**\n",
    "- Loss Computation **PyTorch Loss**\n",
    "- Parameter Update **PyTorch Optimizer**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1\n",
    "\n",
    "- Prediction MANUALLY \n",
    "- Gradients Computation MANUALLY\n",
    "- Loss Computation MANUALLY\n",
    "- Parameter Update MANUALLY\n",
    "\n",
    "We have observations of the next \"unknown\" function:\n",
    "\n",
    "$$\n",
    "f(x) = 2x\n",
    "$$\n",
    "\n",
    "Predictions are calculated as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = wX\n",
    "$$\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} (y-\\hat{y})^T(y-\\hat{y})\n",
    "$$\n",
    "\n",
    "Gradient computation manually:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n}X^T(wX-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import time\n",
    "\n",
    "# training data\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "# weight initialization\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "# loss\n",
    "def loss(y,y_pred):\n",
    "    return ((y-y_pred)**2).mean()\n",
    "\n",
    "# gradient\n",
    "def gradient(x,y,y_pred):\n",
    "    return np.dot(2*x,y_pred-y).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model and learn better parameters $w$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: w = 1.200, loss = 30.00000000\n",
      "Epoch 2/10: w = 1.680, loss = 4.79999924\n",
      "Epoch 3/10: w = 1.872, loss = 0.76800019\n",
      "Epoch 4/10: w = 1.949, loss = 0.12288000\n",
      "Epoch 5/10: w = 1.980, loss = 0.01966083\n",
      "Epoch 6/10: w = 1.992, loss = 0.00314570\n",
      "Epoch 7/10: w = 1.997, loss = 0.00050332\n",
      "Epoch 8/10: w = 1.999, loss = 0.00008053\n",
      "Epoch 9/10: w = 1.999, loss = 0.00001288\n",
      "Epoch 10/10: w = 2.000, loss = 0.00000206\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "total_epochs = 10\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    # prediction (forward pass)\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y,y_pred)\n",
    "\n",
    "    # gradients\n",
    "    dw = gradient(X,y,y_pred)\n",
    "\n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{total_epochs}: w = {w:.3f}, loss = {l:.8f}')\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2\n",
    "\n",
    "- Prediction MANUALLY\n",
    "- Gradients Computation **Autograd**\n",
    "- Loss Computation MANUALLY\n",
    "- Parameter Update MANUALLY\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: w = 0.300, loss = 30.00000000\n",
      "Epoch 2/30: w = 0.555, loss = 21.67499924\n",
      "Epoch 3/30: w = 0.772, loss = 15.66018772\n",
      "Epoch 4/30: w = 0.956, loss = 11.31448650\n",
      "Epoch 5/30: w = 1.113, loss = 8.17471695\n",
      "Epoch 6/30: w = 1.246, loss = 5.90623236\n",
      "Epoch 7/30: w = 1.359, loss = 4.26725292\n",
      "Epoch 8/30: w = 1.455, loss = 3.08308983\n",
      "Epoch 9/30: w = 1.537, loss = 2.22753215\n",
      "Epoch 10/30: w = 1.606, loss = 1.60939169\n",
      "Epoch 11/30: w = 1.665, loss = 1.16278565\n",
      "Epoch 12/30: w = 1.716, loss = 0.84011245\n",
      "Epoch 13/30: w = 1.758, loss = 0.60698116\n",
      "Epoch 14/30: w = 1.794, loss = 0.43854395\n",
      "Epoch 15/30: w = 1.825, loss = 0.31684780\n",
      "Epoch 16/30: w = 1.851, loss = 0.22892261\n",
      "Epoch 17/30: w = 1.874, loss = 0.16539653\n",
      "Epoch 18/30: w = 1.893, loss = 0.11949898\n",
      "Epoch 19/30: w = 1.909, loss = 0.08633806\n",
      "Epoch 20/30: w = 1.922, loss = 0.06237914\n",
      "Epoch 21/30: w = 1.934, loss = 0.04506890\n",
      "Epoch 22/30: w = 1.944, loss = 0.03256231\n",
      "Epoch 23/30: w = 1.952, loss = 0.02352631\n",
      "Epoch 24/30: w = 1.960, loss = 0.01699772\n",
      "Epoch 25/30: w = 1.966, loss = 0.01228084\n",
      "Epoch 26/30: w = 1.971, loss = 0.00887291\n",
      "Epoch 27/30: w = 1.975, loss = 0.00641066\n",
      "Epoch 28/30: w = 1.979, loss = 0.00463169\n",
      "Epoch 29/30: w = 1.982, loss = 0.00334642\n",
      "Epoch 30/30: w = 1.985, loss = 0.00241778\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# training data\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "# weight initialization\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "total_epochs = 30\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    # prediction (forward pass)\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y,y_pred)\n",
    "\n",
    "    # gradients\n",
    "    l.backward() # calculates dl/dw \n",
    "\n",
    "    # update weights\n",
    "    \"\"\"The update of the wieghts w must be done\n",
    "    outside the computational graph. Otherwise,\n",
    "    the update will be like an operation in the\n",
    "    computational graph and the gradients will\n",
    "    be calculated for the update as well. \"\"\"\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    \"\"\"The gradients are written in the w.grad\n",
    "    attribute. We must set them to zero after\n",
    "    each update. Otherwise, the gradients will\n",
    "    be accumulated in the w.grad attribute.\"\"\"\n",
    "    w.grad.zero_() # _ means in-place operation\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{total_epochs}: w = {w:.3f}, loss = {l:.8f}')\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    if l < 0.001:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3\n",
    "\n",
    "- Prediction MANUALLY\n",
    "- Gradients Computation **Autograd**\n",
    "- Loss Computation **PyTorch Loss**\n",
    "- Parameter Update **PyTorch Optimizer**\n",
    "\n",
    "3 usual steps in a PyTorch pipeline:\n",
    "\n",
    "1. Design model (input, output size, forward pass)\n",
    "2. Construct loss and optimizer\n",
    "3. Training loop\n",
    "    - Forward pass: compute prediction and loss\n",
    "    - Backward pass: gradients\n",
    "    - Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: w = 0.300, loss = 30.00000000\n",
      "Epoch 2/30: w = 0.555, loss = 21.67499924\n",
      "Epoch 3/30: w = 0.772, loss = 15.66018772\n",
      "Epoch 4/30: w = 0.956, loss = 11.31448650\n",
      "Epoch 5/30: w = 1.113, loss = 8.17471695\n",
      "Epoch 6/30: w = 1.246, loss = 5.90623236\n",
      "Epoch 7/30: w = 1.359, loss = 4.26725292\n",
      "Epoch 8/30: w = 1.455, loss = 3.08308983\n",
      "Epoch 9/30: w = 1.537, loss = 2.22753215\n",
      "Epoch 10/30: w = 1.606, loss = 1.60939169\n",
      "Epoch 11/30: w = 1.665, loss = 1.16278565\n",
      "Epoch 12/30: w = 1.716, loss = 0.84011245\n",
      "Epoch 13/30: w = 1.758, loss = 0.60698116\n",
      "Epoch 14/30: w = 1.794, loss = 0.43854395\n",
      "Epoch 15/30: w = 1.825, loss = 0.31684780\n",
      "Epoch 16/30: w = 1.851, loss = 0.22892261\n",
      "Epoch 17/30: w = 1.874, loss = 0.16539653\n",
      "Epoch 18/30: w = 1.893, loss = 0.11949898\n",
      "Epoch 19/30: w = 1.909, loss = 0.08633806\n",
      "Epoch 20/30: w = 1.922, loss = 0.06237914\n",
      "Epoch 21/30: w = 1.934, loss = 0.04506890\n",
      "Epoch 22/30: w = 1.944, loss = 0.03256231\n",
      "Epoch 23/30: w = 1.952, loss = 0.02352631\n",
      "Epoch 24/30: w = 1.960, loss = 0.01699772\n",
      "Epoch 25/30: w = 1.966, loss = 0.01228084\n",
      "Epoch 26/30: w = 1.971, loss = 0.00887291\n",
      "Epoch 27/30: w = 1.975, loss = 0.00641066\n",
      "Epoch 28/30: w = 1.979, loss = 0.00463169\n",
      "Epoch 29/30: w = 1.982, loss = 0.00334642\n",
      "Epoch 30/30: w = 1.985, loss = 0.00241778\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# weight initialization\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "pytorch_loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], # parameters to learn\n",
    "                            lr = 0.01) # learning rate\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    \n",
    "    # prediction (forward pass)\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = pytorch_loss(y,y_pred)\n",
    "\n",
    "    # gradients\n",
    "    l.backward() # calculates dl/dw \n",
    "\n",
    "    # update weights\n",
    "    \"\"\" Instead of:\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    we can use the optimizer.step() method \"\"\"\n",
    "    optimizer.step()\n",
    "\n",
    "    # reset gradients\n",
    "    \"\"\" Instead of:\n",
    "    w.grad.zero_()\n",
    "    we can use the optimizer.zero_grad() method \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{total_epochs}: w = {w:.3f}, loss = {l:.8f}')\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    if l < 0.001:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4\n",
    "\n",
    "- Prediction **PyTorch Model**\n",
    "- Gradients Computation **Autograd**\n",
    "- Loss Computation **PyTorch Loss**\n",
    "- Parameter Update **PyTorch Optimizer**\n",
    "\n",
    "We don't need to specify the forward function since it will be implicit in the model we select. We also don't need to define the weights $w$ since they will also be included in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: w = -0.130, b = 0.45 loss = 42.3191\n",
      "Epoch 2/30: w = 0.167, b = 0.55 loss = 29.4204\n",
      "Epoch 3/30: w = 0.414, b = 0.63 loss = 20.4699\n",
      "Epoch 4/30: w = 0.621, b = 0.70 loss = 14.2591\n",
      "Epoch 5/30: w = 0.793, b = 0.75 loss = 9.9491\n",
      "Epoch 6/30: w = 0.936, b = 0.80 loss = 6.9582\n",
      "Epoch 7/30: w = 1.056, b = 0.83 loss = 4.8826\n",
      "Epoch 8/30: w = 1.156, b = 0.86 loss = 3.4420\n",
      "Epoch 9/30: w = 1.239, b = 0.89 loss = 2.4421\n",
      "Epoch 10/30: w = 1.309, b = 0.91 loss = 1.7480\n",
      "Epoch 11/30: w = 1.367, b = 0.93 loss = 1.2660\n",
      "Epoch 12/30: w = 1.416, b = 0.94 loss = 0.9313\n",
      "Epoch 13/30: w = 1.456, b = 0.95 loss = 0.6987\n",
      "Epoch 14/30: w = 1.490, b = 0.96 loss = 0.5370\n",
      "Epoch 15/30: w = 1.519, b = 0.96 loss = 0.4245\n",
      "Epoch 16/30: w = 1.543, b = 0.97 loss = 0.3461\n",
      "Epoch 17/30: w = 1.563, b = 0.97 loss = 0.2914\n",
      "Epoch 18/30: w = 1.580, b = 0.98 loss = 0.2531\n",
      "Epoch 19/30: w = 1.594, b = 0.98 loss = 0.2263\n",
      "Epoch 20/30: w = 1.606, b = 0.98 loss = 0.2074\n",
      "Epoch 21/30: w = 1.616, b = 0.98 loss = 0.1939\n",
      "Epoch 22/30: w = 1.625, b = 0.98 loss = 0.1843\n",
      "Epoch 23/30: w = 1.632, b = 0.98 loss = 0.1773\n",
      "Epoch 24/30: w = 1.639, b = 0.98 loss = 0.1722\n",
      "Epoch 25/30: w = 1.644, b = 0.97 loss = 0.1683\n",
      "Epoch 26/30: w = 1.649, b = 0.97 loss = 0.1654\n",
      "Epoch 27/30: w = 1.653, b = 0.97 loss = 0.1630\n",
      "Epoch 28/30: w = 1.656, b = 0.97 loss = 0.1611\n",
      "Epoch 29/30: w = 1.660, b = 0.97 loss = 0.1595\n",
      "Epoch 30/30: w = 1.662, b = 0.96 loss = 0.1581\n"
     ]
    }
   ],
   "source": [
    "\"Training data must have the shape (n_samples, n_features).\"\n",
    "X = torch.tensor([[1], # 4 samples of 1 feature \n",
    "                  [2],\n",
    "                  [3],\n",
    "                  [4]],\n",
    "                  dtype=torch.float32)\n",
    "y = torch.tensor([[2], # 4 outputs of dim=1\n",
    "                  [4],\n",
    "                  [6],\n",
    "                  [8]],\n",
    "                  dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "input_size = n_features\n",
    "output_size = y.shape[1]\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "        \n",
    "    # prediction (forward pass)\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = pytorch_loss(y,y_pred)\n",
    "\n",
    "    # gradients\n",
    "    l.backward() # calculates dl/dw \n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    [w,b] = model.parameters()\n",
    "    print(f'Epoch {epoch+1}/{total_epochs}: w = {w.item():.3f}, b = {b.item():.2f} loss = {l:.4f}')\n",
    "\n",
    "    if l < 0.001:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom linear regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "        # forward pass\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "\n",
    "newmodel = LinearRegression(input_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8375bd4984629add8d44ff60e70bea75a8303b3cea9050f50a9c41ff6ab03447"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
